{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "stop_words = list(ENGLISH_STOP_WORDS)\n",
    "\n",
    "import pprint\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv('fake_or_real_news.csv', index_col=0)\n",
    "kaggle= pd.read_csv('fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6335, 3), (12999, 20))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, kaggle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cleaning for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:41\n",
      "08:47\n",
      "01:41\n",
      "05:22\n",
      "21:56\n",
      "16:31\n",
      "19:40\n",
      "01:19\n",
      "23:54\n",
      "02:43\n"
     ]
    }
   ],
   "source": [
    "for i in kaggle['published'][:10]:\n",
    "    print(i[11:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2643     2016-11-09T13:48:46.757+02:00\n",
       "10320    2016-11-03T21:25:00.000+02:00\n",
       "Name: published, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle['published'].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_time(kaggle['published'][2643])>get_time(kaggle['published'][10320])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.time(21, 25)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_time(kaggle['published'][10320])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13:48'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "time=datetime.strptime(time, '%H:%M').time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.time(13, 48)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time(time):\n",
    "    '''Formats time from published column'''\n",
    "    time=time[11:16]\n",
    "    time=datetime.strptime(time, '%H:%M').time()\n",
    "    \n",
    "    return time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>ord_in_thread</th>\n",
       "      <th>author</th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>crawled</th>\n",
       "      <th>site_url</th>\n",
       "      <th>country</th>\n",
       "      <th>domain_rank</th>\n",
       "      <th>thread_title</th>\n",
       "      <th>spam_score</th>\n",
       "      <th>main_img_url</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>participants_count</th>\n",
       "      <th>likes</th>\n",
       "      <th>comments</th>\n",
       "      <th>shares</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8575</th>\n",
       "      <td>01b407046f36ececa2279603595bdf9105a31e67</td>\n",
       "      <td>0</td>\n",
       "      <td>Gillian</td>\n",
       "      <td>2016-11-11T20:27:36.711+02:00</td>\n",
       "      <td>The Art of Jumping Timelines</td>\n",
       "      <td>Leave a reply Tom Kenyon \\nThe Hathors – Altho...</td>\n",
       "      <td>english</td>\n",
       "      <td>2016-11-11T20:27:36.711+02:00</td>\n",
       "      <td>shiftfrequency.com</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Art of Jumping Timelines</td>\n",
       "      <td>0.0</td>\n",
       "      <td>http://www.shiftfrequency.com/wp-content/uploa...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8082</th>\n",
       "      <td>4457f1f51bf1f520d045e472247782fc5d927aa9</td>\n",
       "      <td>0</td>\n",
       "      <td>admin</td>\n",
       "      <td>2016-10-27T14:20:06.260+03:00</td>\n",
       "      <td>Jews ‘blamed for Holocaust’ at House of Lords ...</td>\n",
       "      <td>Jews ‘blamed for Holocaust’ at House of Lords ...</td>\n",
       "      <td>english</td>\n",
       "      <td>2016-10-27T14:20:06.260+03:00</td>\n",
       "      <td>rinf.com</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jews ‘blamed for Holocaust’ at House of Lords ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>http://rinf.com/alt-news/wp-content/uploads/20...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4601</th>\n",
       "      <td>3b83ca9258f3963db7613986384601947a14e15e</td>\n",
       "      <td>0</td>\n",
       "      <td>Mr. Wendal</td>\n",
       "      <td>2016-10-27T21:50:31.139+03:00</td>\n",
       "      <td>Homeless Trump Supporter Posts As GUARD For Tr...</td>\n",
       "      <td>0 comments \\nWOW! Liberals can’t even let a do...</td>\n",
       "      <td>english</td>\n",
       "      <td>2016-10-27T21:50:31.139+03:00</td>\n",
       "      <td>ihavethetruth.com</td>\n",
       "      <td>US</td>\n",
       "      <td>67400.0</td>\n",
       "      <td>Homeless Trump Supporter Posts As GUARD For Tr...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://ihavethetruth.com/wp-content/uploads/2...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          uuid  ord_in_thread      author  \\\n",
       "8575  01b407046f36ececa2279603595bdf9105a31e67              0     Gillian   \n",
       "8082  4457f1f51bf1f520d045e472247782fc5d927aa9              0       admin   \n",
       "4601  3b83ca9258f3963db7613986384601947a14e15e              0  Mr. Wendal   \n",
       "\n",
       "                          published  \\\n",
       "8575  2016-11-11T20:27:36.711+02:00   \n",
       "8082  2016-10-27T14:20:06.260+03:00   \n",
       "4601  2016-10-27T21:50:31.139+03:00   \n",
       "\n",
       "                                                  title  \\\n",
       "8575                       The Art of Jumping Timelines   \n",
       "8082  Jews ‘blamed for Holocaust’ at House of Lords ...   \n",
       "4601  Homeless Trump Supporter Posts As GUARD For Tr...   \n",
       "\n",
       "                                                   text language  \\\n",
       "8575  Leave a reply Tom Kenyon \\nThe Hathors – Altho...  english   \n",
       "8082  Jews ‘blamed for Holocaust’ at House of Lords ...  english   \n",
       "4601  0 comments \\nWOW! Liberals can’t even let a do...  english   \n",
       "\n",
       "                            crawled            site_url country  domain_rank  \\\n",
       "8575  2016-11-11T20:27:36.711+02:00  shiftfrequency.com      US          NaN   \n",
       "8082  2016-10-27T14:20:06.260+03:00            rinf.com      US          NaN   \n",
       "4601  2016-10-27T21:50:31.139+03:00   ihavethetruth.com      US      67400.0   \n",
       "\n",
       "                                           thread_title  spam_score  \\\n",
       "8575                       The Art of Jumping Timelines         0.0   \n",
       "8082  Jews ‘blamed for Holocaust’ at House of Lords ...         0.0   \n",
       "4601  Homeless Trump Supporter Posts As GUARD For Tr...         0.0   \n",
       "\n",
       "                                           main_img_url  replies_count  \\\n",
       "8575  http://www.shiftfrequency.com/wp-content/uploa...              0   \n",
       "8082  http://rinf.com/alt-news/wp-content/uploads/20...              0   \n",
       "4601  https://ihavethetruth.com/wp-content/uploads/2...              0   \n",
       "\n",
       "      participants_count  likes  comments  shares type  \n",
       "8575                   1      0         0       0   bs  \n",
       "8082                   1      0         0       0   bs  \n",
       "4601                   1      0         0       0   bs  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.unique of bs            11492\n",
       "bias            443\n",
       "conspiracy      430\n",
       "hate            246\n",
       "satire          146\n",
       "state           121\n",
       "junksci         102\n",
       "fake             19\n",
       "Name: type, dtype: int64>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle['type'].value_counts().unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kaggle[\"label\"]=kaggle[\"type\"]\n",
    "kaggle=kaggle.drop('type', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le1 = LabelEncoder()\n",
    "kaggle['numerical_label']=le1.fit_transform(kaggle['label'])\n",
    "kaggle = kaggle.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kaggle.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.sample(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['label'].value_counts().unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le2 = LabelEncoder()\n",
    "df['binary_label']=le2.fit_transform(df['label'])\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    try:\n",
    "        text = re.sub('[.,\\/#!$%\\^&\\*;:{}\\+=<>_`~()]', ' ', text)\n",
    "        text = re.sub('[^a-z0-9 ]','', text.lower())\n",
    "#         text = re.sub('displaystyle',' ', text)\n",
    "        text = re.sub('\\s+',' ',text)\n",
    "    except: \n",
    "        pass\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"text\"]=df[\"text\"].apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.sample(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas to drive from EDA:\n",
    "* Look at global token counts\n",
    "  * Ngrams (individually 2,3,4,5,6.  Then Range 2,6)\n",
    "* Look at subsets for labels (+ ngrams)\n",
    "* Look at New datasets (kaggle)\n",
    "* Look at specific time periods (do some research first)\n",
    "* Look at what is distinct between classes / labels\n",
    "* Make this modular (consider OO python)\n",
    "  * Swap out vectorizers\n",
    "* Start plotting everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# stop_words.extend([\"number\", \"like\", \"just\"]) #after round one\n",
    "# stop_words.extend([\"donald\",\"trump\", \"hillary\", \"clinton\", \"obama\", \"number\", \"like\"])\n",
    "#\"united\", \"states\", \"white\", \"house\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_stops=[\"donald\",\"trump\", \"clinton\", \"obama\", \"number\", \"like\", \"number\", \"like\", \"just\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TokenAnalysis(object):\n",
    "    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "    \n",
    "    \n",
    "    def __init__(self, df, Vectorizer=None, max_df=None, n_gram_range=None, stop_words=None):\n",
    "        self.text=df[\"text\"]\n",
    "        self.label=df[\"label\"]\n",
    "        \n",
    "        self.max_df=max_df\n",
    "        self.ngram_range= n_gram_range\n",
    "        self.vec=Vectorizer\n",
    "#         self.vectorizer=self.init_vectorizer(sel.vec)\n",
    "        \n",
    "        if stop_words is None:\n",
    "            self.stop_words=list(ENGLISH_STOP_WORDS)           \n",
    "        else:\n",
    "            self.stop_words= stop_words\n",
    "        \n",
    "        \n",
    "        if self.vec is None:\n",
    "            if self.max_df is None:\n",
    "                self.vectorizer=CountVectorizer(stop_words=self.stop_words, max_df=0.7) \n",
    "            else:\n",
    "                self.vectorizer=CountVectorizer(stop_words=self.stop_words, max_df=self.max_df)\n",
    "                \n",
    "        else:                    \n",
    "            if self.ngram_range is None:\n",
    "                self.vectorizer=self.vec(stop_words=self.stop_words, max_df=0.7)\n",
    "            else:\n",
    "                self.vectorizer=self.vec(stop_words=self.stop_words, max_df=0.7, ngram_range=self.ngram_range)\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "        # make token dataframe called X_text\n",
    "        self.X = self.vectorizer.fit_transform(self.text)\n",
    "        self.X_text = pd.DataFrame(self.X.toarray(), columns = self.vectorizer.get_feature_names())\n",
    "        self.X_text.index=self.label\n",
    "    \n",
    "    #intialize vectorizer\n",
    "#     def init_vectorizer(self, vec):   \n",
    "         \n",
    "#         #default to CountVectorizer\n",
    "\n",
    "#         if self.vec is None:\n",
    "#             if self.max_df is None:\n",
    "#                 self.vectorizer=CountVectorizer(stop_words=self.stop_words, max_df=0.7) \n",
    "#             else:\n",
    "#                 self.vectorizer=CountVectorizer(stop_words=self.stop_words, max_df=self.max_df)\n",
    "                \n",
    "#         else:                    \n",
    "#             if self.ngram_range is None:\n",
    "#                 self.vectorizer=self.vec(stop_words=self.stop_words, max_df=0.7)\n",
    "#             else:\n",
    "#                 self.vectorizer=self.vec(stop_words=self.stop_words, max_df=0.7, ngram_range=self.ngram_range)\n",
    "                \n",
    "        \n",
    "    def top_tokens(self, num=None, label=None):\n",
    "        \n",
    "        if num is None and label is None:\n",
    "            token_sum = self.X_text.sum()\n",
    "            top=token_sum.sort_values(ascending=False)\n",
    "            return top[:10]\n",
    "        \n",
    "        if label is None:\n",
    "            token_sum = self.X_text.sum()\n",
    "            top=token_sum.sort_values(ascending=False)\n",
    "            return top[:num]\n",
    "        \n",
    "        else:\n",
    "            token_sums_labeled = self.X_text[self.X_text.index==label].sum()\n",
    "            top=token_sums_labeled.sort_values(ascending=False)\n",
    "            \n",
    "            if num is None:\n",
    "                return top[:10]\n",
    "            else:\n",
    "                return top[:num] \n",
    "            \n",
    "            \n",
    "            \n",
    "    def top_tokens_label_freq(self, num=None):\n",
    "        if num is None:\n",
    "            num=10\n",
    "            \n",
    "        top=self.top_tokens(num)\n",
    "        return self.X_text.groupby(self.X_text.index)[top.index[:num]].sum()                                                      \n",
    "\n",
    "    \n",
    "    def plot_top_tokens(self, num=None, label=None):\n",
    "        top=self.top_tokens(num, label)\n",
    "     \n",
    "        if label is None:\n",
    "            label=\"GLOBAL\"\n",
    "        \n",
    "        plt.figure(figsize=(8,6))\n",
    "        \n",
    "        plot1=sns.barplot(x=top.index, y=top)\n",
    "        \n",
    "        plot1.set_title(\"Top {} {} tokens\".format(len(top), label))\n",
    "        plot1.set_xticklabels(top.index, rotation=30)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def plot_top_tokens_by_binary_label(self, num=None):\n",
    "        #set scale by axis\n",
    "        labeled_df=self.top_tokens_label_freq(num).T\n",
    "        labels=list(labeled_df.columns)\n",
    "        ylim=max(max(self.top_tokens(label=labels[0])), max(self.top_tokens(label=labels[1])))\n",
    "        \n",
    "        fig, ax = plt.subplots(1, len(labels), figsize=(15, 8))\n",
    "\n",
    "        for axes in range(len(labels)):\n",
    "            label=labels[axes]\n",
    "            plot=labeled_df[label].plot(kind=\"bar\", ax=ax[axes])\n",
    "            plot.set_xticklabels(labeled_df.index, rotation=30)\n",
    "            ax[axes].set_title(\"Freq of top tokens in {} Articles\".format(label))\n",
    "            ax[axes].set_ylim([0,ylim*1.10])\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def plot_top_tokens_by_label(self, num=None):\n",
    "        labeled_df=self.top_tokens_label_freq(num).T\n",
    "        labels=list(labeled_df.columns)\n",
    "        ylim=0\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            if labels[i] != \"bs\":\n",
    "                ylim= max(ylim, max(self.top_tokens(label=labels[i])))\n",
    "        \n",
    "        fig, ax = plt.subplots(len(labels), 1, figsize=(10, 75))\n",
    "\n",
    "        for axes in range(len(labels)):\n",
    "            label=labels[axes]\n",
    "            plot=labeled_df[label].plot(kind=\"bar\", ax=ax[axes])\n",
    "            plot.set_xticklabels(labeled_df.index, rotation=30)\n",
    "            ax[axes].set_title(\"Freq of top tokens in {} Articles\".format(label))\n",
    "            if label!=\"bs\":\n",
    "                ax[axes].set_ylim([0,ylim*1.10])\n",
    "\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def add_stops(self, words):\n",
    "        if type(words) is list:\n",
    "            self.stop_words.extend(words)\n",
    "        else:\n",
    "            self.stop_words.append(words)\n",
    "        \n",
    "\n",
    "        if self.vec is None:\n",
    "            if self.max_df is None:\n",
    "                self.vectorizer=CountVectorizer(stop_words=self.stop_words, max_df=0.7) \n",
    "            else:\n",
    "                self.vectorizer=CountVectorizer(stop_words=self.stop_words, max_df=self.max_df)\n",
    "                \n",
    "        else:                    \n",
    "            if self.ngram_range is None:\n",
    "                self.vectorizer=self.vec(stop_words=self.stop_words, max_df=0.7)\n",
    "            else:\n",
    "                self.vectorizer=self.vec(stop_words=self.stop_words, max_df=0.7, ngram_range=self.ngram_range)\n",
    "        \n",
    "        \n",
    "        self.X = self.vectorizer.fit_transform(self.text)\n",
    "        self.X_text = pd.DataFrame(self.X.toarray(), columns = self.vectorizer.get_feature_names())\n",
    "        self.X_text.index=self.label\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####kaggle token analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kaggle['label'].value_counts().unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kaggle['text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_date(date):\n",
    "    date=date[:10]\n",
    "    date=datetime.strptime(date, '%Y-%m-%d')\n",
    "    \n",
    "    return date\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kaggle[\"date\"] = kaggle['published'].apply(get_date)\n",
    "kaggle[\"date\"] = pd.to_datetime(kaggle[\"date\"])\n",
    "kaggle[\"month\"] = kaggle[\"date\"].dt.month\n",
    "kaggle[\"day\"] = kaggle[\"date\"].dt.day\n",
    "kaggle[\"year\"] = kaggle[\"date\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean text\n",
    "kaggle[\"text\"]=kaggle[\"text\"].apply(cleaner)\n",
    "kaggle[\"text\"] = kaggle[\"text\"].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add fake to bs\n",
    "kaggle.loc[kaggle['label'] == \"fake\", 'label'] = \"bs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kaggle['label'].value_counts().unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kag_sub=kaggle.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Date Range: \", (str(min(kaggle[\"date\"])), max(kaggle[\"date\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feature engineering: make social reach score from social reach cols\n",
    "social_reach_cols=[\"replies_count\", 'participants_count', 'likes', 'comments','shares']\n",
    "feature_engineer_df=kaggle[social_reach_cols]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "scaler=MinMaxScaler()\n",
    "for col in feature_engineer_df.columns:\n",
    "    x_scaled = scaler.fit_transform(feature_engineer_df[social_reach_cols])\n",
    "    feature_engineer_df[col]=x_scaled\n",
    "\n",
    "\n",
    "kaggle[\"social_reach_score\"]=feature_engineer_df[social_reach_cols[0]] + feature_engineer_df[social_reach_cols[1]] \\\n",
    "                        + feature_engineer_df[social_reach_cols[2]]  + feature_engineer_df[social_reach_cols[3]] \\\n",
    "                        + feature_engineer_df[social_reach_cols[4]] \n",
    "kaggle.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dataframes to model\n",
    "\n",
    "\n",
    "#1 4 distinct labels\n",
    "kaggle=kaggle[(kaggle[\"label\"]==\"bs\")\\\n",
    "       | (kaggle[\"label\"]==\"bias\") \\\n",
    "       | (kaggle[\"label\"]==\"conspiracy\") \\\n",
    "       | (kaggle[\"label\"]==\"satire\")]\n",
    "\n",
    "\n",
    "#2 Important date ranges between 10-26 and 11-25\n",
    "before_comey_mask=kaggle[\"date\"]<'2016-10-28 00:00:00'\n",
    "comey_mask=(kaggle[\"date\"]>='2016-10-28 00:00:00' )& (kaggle[\"date\"]<='2016-11-07 00:00:00')\n",
    "election_mask=(kaggle[\"date\"]>='2016-11-08 00:00:00' )& (kaggle[\"date\"]<='2016-11-10 00:00:00')\n",
    "after_election_mask=kaggle[\"date\"]>='2016-11-10 00:00:00' \n",
    "\n",
    "bc_df = kaggle[before_comey_mask]\n",
    "comey_df = kaggle[comey_mask]\n",
    "election_df = kaggle[election_mask]\n",
    "ae_df = kaggle[after_election_mask]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Before Comey letter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CV_bc = TokenAnalysis(bc_df)\n",
    "TF_1_ngram_bc = TokenAnalysis(bc_df, TfidfVectorizer)\n",
    "TF_2_ngram_bc = TokenAnalysis(bc_df, TfidfVectorizer, n_gram_range=(2,2))\n",
    "TF_3_ngram_bc = TokenAnalysis(bc_df, TfidfVectorizer, n_gram_range=(3,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 GLOBAL tokens\n",
    "CV_bc.plot_top_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 GLOBAL tokens for each label\n",
    "\n",
    "CV_bc.plot_top_tokens_by_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 bs tokens\n",
    "CV_bc.plot_top_tokens(label=\"bs\")\n",
    "\n",
    "# Top 10 bias tokens\n",
    "CV_bc.plot_top_tokens(label=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 conspiracy tokens\n",
    "CV_bc.plot_top_tokens(label=\"conspiracy\")\n",
    "\n",
    "# Top 10 satire tokens\n",
    "CV_bc.plot_top_tokens(label=\"satire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 GLOBAL tokens\n",
    "TF_2_ngram_bc.plot_top_tokens()\n",
    "\n",
    "# Top 10 GLOBAL tokens for each label\n",
    "TF_2_ngram_bc.plot_top_tokens_by_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 bs tokens\n",
    "TF_2_ngram_bc.plot_top_tokens(label=\"bs\")\n",
    "\n",
    "# Top 10 bias tokens\n",
    "TF_2_ngram_bc.plot_top_tokens(label=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 conspiracy tokens\n",
    "TF_2_ngram_bc.plot_top_tokens(label=\"conspiracy\")\n",
    "\n",
    "# Top 10 satire tokens\n",
    "TF_2_ngram_bc.plot_top_tokens(label=\"satire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 GLOBAL tokens\n",
    "TF_3_ngram_bc.plot_top_tokens()\n",
    "\n",
    "# Top 10 GLOBAL tokens for each label\n",
    "TF_3_ngram_bc.plot_top_tokens_by_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 bs tokens\n",
    "TF_3_ngram_bc.plot_top_tokens(label=\"bs\")\n",
    "\n",
    "# Top 10 bias tokens\n",
    "TF_3_ngram_bc.plot_top_tokens(label=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 conspiracy tokens\n",
    "TF_3_ngram_bc.plot_top_tokens(label=\"conspiracy\")\n",
    "\n",
    "# Top 10 satire tokens\n",
    "TF_3_ngram_bc.plot_top_tokens(label=\"satire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# During Comey letter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CV_comey = TokenAnalysis(comey_df)\n",
    "TF_2_ngram_comey = TokenAnalysis(comey_df, TfidfVectorizer, n_gram_range=(2,2))\n",
    "TF_3_ngram_comey = TokenAnalysis(comey_df, TfidfVectorizer, n_gram_range=(3,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 GLOBAL tokens\n",
    "CV_comey.plot_top_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 GLOBAL tokens for each label\n",
    "\n",
    "CV_comey.plot_top_tokens_by_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 bs tokens\n",
    "CV_comey.plot_top_tokens(label=\"bs\")\n",
    "\n",
    "# Top 10 bias tokens\n",
    "CV_comey.plot_top_tokens(label=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 conspiracy tokens\n",
    "CV_comey.plot_top_tokens(label=\"conspiracy\")\n",
    "\n",
    "# Top 10 satire tokens\n",
    "CV_comey.plot_top_tokens(label=\"satire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 GLOBAL tokens\n",
    "TF_2_ngram_comey.plot_top_tokens()\n",
    "\n",
    "# Top 10 GLOBAL tokens for each label\n",
    "TF_2_ngram_comey.plot_top_tokens_by_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 bs tokens\n",
    "TF_2_ngram_comey.plot_top_tokens(label=\"bs\")\n",
    "\n",
    "# Top 10 bias tokens\n",
    "TF_2_ngram_comey.plot_top_tokens(label=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 conspiracy tokens\n",
    "TF_2_ngram_comey.plot_top_tokens(label=\"conspiracy\")\n",
    "\n",
    "# Top 10 satire tokens\n",
    "TF_2_ngram_comey.plot_top_tokens(label=\"satire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 GLOBAL tokens\n",
    "TF_3_ngram_comey.plot_top_tokens()\n",
    "\n",
    "# Top 10 GLOBAL tokens for each label\n",
    "TF_3_ngram_comey.plot_top_tokens_by_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 bs tokens\n",
    "TF_3_ngram_comey.plot_top_tokens(label=\"bs\")\n",
    "\n",
    "# Top 10 bias tokens\n",
    "TF_3_ngram_comey.plot_top_tokens(label=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 conspiracy tokens\n",
    "TF_3_ngram_comey.plot_top_tokens(label=\"conspiracy\")\n",
    "\n",
    "# Top 10 satire tokens\n",
    "TF_3_ngram_comey.plot_top_tokens(label=\"satire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### During Election"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CV_election = TokenAnalysis(election_df)\n",
    "TF_2_ngram_election = TokenAnalysis(election_df, TfidfVectorizer, n_gram_range=(2,2))\n",
    "TF_3_ngram_election = TokenAnalysis(election_df, TfidfVectorizer, n_gram_range=(3,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 GLOBAL tokens\n",
    "CV_election.plot_top_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 GLOBAL tokens for each label\n",
    "\n",
    "CV_election.plot_top_tokens_by_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 bs tokens\n",
    "CV_election.plot_top_tokens(label=\"bs\")\n",
    "\n",
    "# Top 10 bias tokens\n",
    "CV_election.plot_top_tokens(label=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 conspiracy tokens\n",
    "CV_election.plot_top_tokens(label=\"conspiracy\")\n",
    "\n",
    "# Top 10 satire tokens\n",
    "CV_election.plot_top_tokens(label=\"satire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 GLOBAL tokens\n",
    "TF_2_ngram_election.plot_top_tokens()\n",
    "\n",
    "# Top 10 GLOBAL tokens for each label\n",
    "TF_2_ngram_election.plot_top_tokens_by_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 bs tokens\n",
    "TF_2_ngram_election.plot_top_tokens(label=\"bs\")\n",
    "\n",
    "# Top 10 bias tokens\n",
    "TF_2_ngram_election.plot_top_tokens(label=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 conspiracy tokens\n",
    "TF_2_ngram_election.plot_top_tokens(label=\"conspiracy\")\n",
    "\n",
    "# Top 10 satire tokens\n",
    "TF_2_ngram_election.plot_top_tokens(label=\"satire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 GLOBAL tokens\n",
    "TF_3_ngram_election.plot_top_tokens()\n",
    "\n",
    "# Top 10 GLOBAL tokens for each label\n",
    "TF_3_ngram_election.plot_top_tokens_by_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 bs tokens\n",
    "TF_3_ngram_election.plot_top_tokens(label=\"bs\")\n",
    "\n",
    "# Top 10 bias tokens\n",
    "TF_3_ngram_election.plot_top_tokens(label=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 conspiracy tokens\n",
    "TF_3_ngram_election.plot_top_tokens(label=\"conspiracy\")\n",
    "\n",
    "# Top 10 satire tokens\n",
    "TF_3_ngram_election.plot_top_tokens(label=\"satire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### After election"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CV_ae = TokenAnalysis(ae_df)\n",
    "TF_2_ngram_ae = TokenAnalysis(ae_df, TfidfVectorizer, n_gram_range=(2,2))\n",
    "TF_3_ngram_ae = TokenAnalysis(ae_df, TfidfVectorizer, n_gram_range=(3,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 GLOBAL tokens\n",
    "CV_ae.plot_top_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 GLOBAL tokens for each label\n",
    "\n",
    "CV_ae.plot_top_tokens_by_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 bs tokens\n",
    "CV_ae.plot_top_tokens(label=\"bs\")\n",
    "\n",
    "# Top 10 bias tokens\n",
    "CV_ae.plot_top_tokens(label=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 conspiracy tokens\n",
    "CV_ae.plot_top_tokens(label=\"conspiracy\")\n",
    "\n",
    "# Top 10 satire tokens\n",
    "CV_ae.plot_top_tokens(label=\"satire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 GLOBAL tokens\n",
    "TF_2_ngram_ae.plot_top_tokens()\n",
    "\n",
    "# Top 10 GLOBAL tokens for each label\n",
    "TF_2_ngram_ae.plot_top_tokens_by_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 bs tokens\n",
    "TF_2_ngram_ae.plot_top_tokens(label=\"bs\")\n",
    "\n",
    "# Top 10 bias tokens\n",
    "TF_2_ngram_ae.plot_top_tokens(label=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 conspiracy tokens\n",
    "TF_2_ngram_ae.plot_top_tokens(label=\"conspiracy\")\n",
    "\n",
    "# Top 10 satire tokens\n",
    "TF_2_ngram_ae.plot_top_tokens(label=\"satire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 GLOBAL tokens\n",
    "TF_3_ngram_ae.plot_top_tokens()\n",
    "\n",
    "# Top 10 GLOBAL tokens for each label\n",
    "TF_3_ngram_ae.plot_top_tokens_by_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 bs tokens\n",
    "TF_3_ngram_ae.plot_top_tokens(label=\"bs\")\n",
    "\n",
    "# Top 10 bias tokens\n",
    "TF_3_ngram_ae.plot_top_tokens(label=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 10 conspiracy tokens\n",
    "TF_3_ngram_ae.plot_top_tokens(label=\"conspiracy\")\n",
    "\n",
    "# Top 10 satire tokens\n",
    "TF_3_ngram_ae.plot_top_tokens(label=\"satire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the `tfidf_vectorizer`  with n_gram range 2,6\n",
    "# tfidf_vectorizer2 = TfidfVectorizer(stop_words=stop_words, max_df=0.7, ngram_range=(2,2)) \n",
    "\n",
    "# # Fit and transform the training data \n",
    "# tfidf_train2 = tfidf_vectorizer2.fit_transform(X_train) \n",
    "\n",
    "# # Transform the test set \n",
    "# tfidf_test2 = tfidf_vectorizer2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clf = MultinomialNB()\n",
    "\n",
    "# clf.fit(count_train, y_train)\n",
    "# pred = clf.predict(count_test)\n",
    "# score = accuracy_score(y_test, pred)\n",
    "# print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clf = MultinomialNB()\n",
    "\n",
    "# clf.fit(tfidf_train, y_train)\n",
    "# pred = clf.predict(tfidf_test)\n",
    "# score = accuracy_score(y_test, pred)\n",
    "# print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
